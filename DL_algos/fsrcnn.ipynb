{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-03T13:11:05.994918Z",
     "start_time": "2025-12-03T13:11:05.931014Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import classic_algos.bicubic_interpolation as bicubic\n",
    "import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T13:11:06.058564Z",
     "start_time": "2025-12-03T13:11:06.035984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SatelliteSRDataset(Dataset):\n",
    "    def __init__(self, root_dir, scale_factor=3, hr_patch_size=144, augment=True):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.scale_factor = scale_factor\n",
    "        self.hr_patch_size = hr_patch_size\n",
    "        self.augment = augment\n",
    "\n",
    "        self.file_paths = glob.glob(os.path.join(root_dir, '**', '*.tif'), recursive=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.file_paths[idx]\n",
    "        with rasterio.open(img_path) as src:\n",
    "            image = src.read()\n",
    "\n",
    "        image_norm = image.astype(np.float32) / 255.0\n",
    "\n",
    "        #random crop\n",
    "        c, h, w = image_norm.shape\n",
    "        top = random.randint(0, h - self.hr_patch_size)\n",
    "        left = random.randint(0, w - self.hr_patch_size)\n",
    "        hr_patch_np = image_norm[:, top:top+self.hr_patch_size, left:left+self.hr_patch_size]\n",
    "        if self.augment:\n",
    "            # Flip Horizontal\n",
    "            if random.random() < 0.5:\n",
    "                hr_patch_np = np.flip(hr_patch_np, axis=2)\n",
    "\n",
    "            # Flip Vertical\n",
    "            if random.random() < 0.5:\n",
    "                hr_patch_np = np.flip(hr_patch_np, axis=1)\n",
    "\n",
    "            # rotations\n",
    "            k = random.choice([0, 1, 2, 3])\n",
    "            if k > 0:\n",
    "                hr_patch_np = np.rot90(hr_patch_np, k, axes=(1, 2))\n",
    "\n",
    "        hr_patch_np = np.ascontiguousarray(hr_patch_np)\n",
    "        hr_tensor = torch.from_numpy(hr_patch_np)\n",
    "\n",
    "        hr_patch_hwc = image[:, top:top+self.hr_patch_size, left:left+self.hr_patch_size].transpose(1, 2, 0)\n",
    "        lr_size = self.hr_patch_size // self.scale_factor\n",
    "        lr_numpy_hwc = bicubic.SR_bicubic(hr_patch_hwc, lr_size, lr_size)\n",
    "        lr_numpy = np.transpose(lr_numpy_hwc, (2, 0, 1)).astype(np.float32) / 255.0\n",
    "        lr_tensor = torch.from_numpy(lr_numpy)\n",
    "\n",
    "        return lr_tensor, hr_tensor"
   ],
   "id": "ba4bbeb2195c07a1",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T13:11:06.120951Z",
     "start_time": "2025-12-03T13:11:06.109571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FSRCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        d: filters dimension\n",
    "        s: shrinking dimension\n",
    "        m: mapping layers number\n",
    "        \"\"\"\n",
    "        d, s, m = 56, 12, 4\n",
    "        scale_factor = 3\n",
    "        channels = 3\n",
    "        super(FSRCNN, self).__init__()\n",
    "\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(channels, d, kernel_size=5, padding=2),\n",
    "            nn.PReLU(d)\n",
    "        )\n",
    "\n",
    "        self.shrink = nn.Sequential(\n",
    "            nn.Conv2d(d, s, kernel_size=1),\n",
    "            nn.PReLU(s)\n",
    "        )\n",
    "\n",
    "        map_layers = []\n",
    "        for _ in range(m):\n",
    "            map_layers.extend([nn.Conv2d(s, s, kernel_size=3, padding=1),\n",
    "                               nn.PReLU(s)\n",
    "            ])\n",
    "        self.mapping = nn.Sequential(*map_layers)\n",
    "\n",
    "        self.expand = nn.Sequential(\n",
    "            nn.Conv2d(s, d, kernel_size=1),\n",
    "            nn.PReLU(d)\n",
    "        )\n",
    "\n",
    "        self.deconv = nn.ConvTranspose2d(d, channels,\n",
    "                                         kernel_size=9,\n",
    "                                         stride=3,\n",
    "                                         padding=3,\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extraction(x)\n",
    "        x = self.shrink(x)\n",
    "        x = self.mapping(x)\n",
    "        x = self.expand(x)\n",
    "        x = self.deconv(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.normal_(m.weight.data, mean=0.0, std=0.001)\n"
   ],
   "id": "f8c494750ce5bb31",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "train",
   "id": "c3f430706fb49d7d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T13:11:06.175253Z",
     "start_time": "2025-12-03T13:11:06.168734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def PSNR(img1, img2):\n",
    "    mse = torch.mean((img1 - img2) ** 2)\n",
    "    return 10 * torch.log10(1 / (mse + 1e-20))"
   ],
   "id": "d83c6b193526ae89",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T13:11:08.110520Z",
     "start_time": "2025-12-03T13:11:06.225574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from pathlib import Path\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "\n",
    "TRAIN_DIR = Path.home() / '.data' / 'UCMerced_LandUse_Split' / 'train'\n",
    "VAL_DIR = Path.home() / '.data' / 'UCMerced_LandUse_Split' / 'val'\n",
    "MODEL_DIR = Path.home() / '.data' / 'fsrcnn_models'\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n"
   ],
   "id": "4ae429a2f8440780",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T13:11:08.519726Z",
     "start_time": "2025-12-03T13:11:08.473263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = SatelliteSRDataset(TRAIN_DIR)\n",
    "val_dataset = SatelliteSRDataset(VAL_DIR, augment=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ],
   "id": "aa8b7aeabf9d1e8",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T13:11:08.575511Z",
     "start_time": "2025-12-03T13:11:08.539708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = FSRCNN().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ],
   "id": "9ab79d38640e8861",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T13:11:08.945953Z",
     "start_time": "2025-12-03T13:11:08.596809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss = []\n",
    "psnr_metric = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for lr_imgs, hr_imgs in tqdm(train_loader):\n",
    "        lr_imgs = lr_imgs.to(device)\n",
    "        hr_imgs = hr_imgs.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        sr_imgs = model(lr_imgs)\n",
    "        batch_loss = criterion(sr_imgs, hr_imgs)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += batch_loss.item()\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    loss.append(avg_loss)\n",
    "\n",
    "    if epoch % 5 != 0:\n",
    "        continue\n",
    "\n",
    "    model.eval()\n",
    "    epoch_psnr = 0\n",
    "    with torch.no_grad():\n",
    "        for lr_imgs, hr_imgs in val_loader:\n",
    "            lr_imgs = lr_imgs.to(device)\n",
    "            hr_imgs = hr_imgs.to(device)\n",
    "            sr_imgs = model(lr_imgs)\n",
    "            epoch_psnr += PSNR(sr_imgs, hr_imgs).item()\n",
    "    avg_psnr = epoch_psnr / len(val_loader)\n",
    "    psnr_metric.append(avg_psnr)\n",
    "    print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}, PSNR: {avg_psnr:.2f} dB\")\n",
    "    torch.save(model.state_dict(), MODEL_DIR / f'fsrcnn_epoch_{epoch}.pth')\n",
    "\n",
    "print(\"Training complete.\")"
   ],
   "id": "69a9b04f1da07da9",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mImportError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[15]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      5\u001B[39m model.train()\n\u001B[32m      6\u001B[39m epoch_loss = \u001B[32m0\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m lr_imgs, hr_imgs \u001B[38;5;129;01min\u001B[39;00m \u001B[43mtqdm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[32m      8\u001B[39m     lr_imgs = lr_imgs.to(device)\n\u001B[32m      9\u001B[39m     hr_imgs = hr_imgs.to(device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/super-resolution/lib/python3.13/site-packages/tqdm/notebook.py:234\u001B[39m, in \u001B[36mtqdm_notebook.__init__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    232\u001B[39m unit_scale = \u001B[32m1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.unit_scale \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.unit_scale \u001B[38;5;129;01mor\u001B[39;00m \u001B[32m1\u001B[39m\n\u001B[32m    233\u001B[39m total = \u001B[38;5;28mself\u001B[39m.total * unit_scale \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.total \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.total\n\u001B[32m--> \u001B[39m\u001B[32m234\u001B[39m \u001B[38;5;28mself\u001B[39m.container = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstatus_printer\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtotal\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdesc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mncols\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    235\u001B[39m \u001B[38;5;28mself\u001B[39m.container.pbar = proxy(\u001B[38;5;28mself\u001B[39m)\n\u001B[32m    236\u001B[39m \u001B[38;5;28mself\u001B[39m.displayed = \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/super-resolution/lib/python3.13/site-packages/tqdm/notebook.py:108\u001B[39m, in \u001B[36mtqdm_notebook.status_printer\u001B[39m\u001B[34m(_, total, desc, ncols)\u001B[39m\n\u001B[32m     99\u001B[39m \u001B[38;5;66;03m# Fallback to text bar if there's no total\u001B[39;00m\n\u001B[32m    100\u001B[39m \u001B[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001B[39;00m\n\u001B[32m    101\u001B[39m \u001B[38;5;66;03m# if not total:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    105\u001B[39m \n\u001B[32m    106\u001B[39m \u001B[38;5;66;03m# Prepare IPython progress bar\u001B[39;00m\n\u001B[32m    107\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m IProgress \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:  \u001B[38;5;66;03m# #187 #451 #558 #872\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m108\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(WARN_NOIPYW)\n\u001B[32m    109\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m total:\n\u001B[32m    110\u001B[39m     pbar = IProgress(\u001B[38;5;28mmin\u001B[39m=\u001B[32m0\u001B[39m, \u001B[38;5;28mmax\u001B[39m=total)\n",
      "\u001B[31mImportError\u001B[39m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss, label='Train Loss (MSE)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(psnr_metric, label='Val PSNR', color='orange')\n",
    "plt.title('Метрика качества (PSNR)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('dB')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ],
   "id": "2a99ef67c2372d47"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
